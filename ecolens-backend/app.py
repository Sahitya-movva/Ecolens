from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
import pandas as pd
import uuid
import os
import requests
from bs4 import BeautifulSoup

from utils.nlp_logic import analyze_claim_text

app = FastAPI(title="EcoLens Backend API")

# -------------------- CORS --------------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# -------------------- DATASET --------------------
df = pd.read_csv("data/ecolens_claims_master_1000.csv")

# -------------------- ROOT --------------------
@app.get("/")
def root():
    return {"status": "EcoLens backend running"}

# -------------------- ANALYZE TEXT --------------------
@app.post("/analyze-text")
def analyze_text(payload: dict):
    claim_text = payload.get("claim_text", "").strip()

    ai_result = analyze_claim_text(claim_text)

    return {
        "risk": ai_result["risk"],
        "reasons": (
            ai_result["why_flagged"]
            if isinstance(ai_result["why_flagged"], list)
            else [ai_result["why_flagged"]]
        ),
        "type": ai_result["greenwashing_type"],
        "consumerRisk": int(ai_result["consumer_risk_score"]),
        "envConfidence": int(ai_result["environmental_confidence"]),
        "credibility": int(ai_result["credibility_score"]),
        "alternatives": [
            "Prefer products certified by FSC, EcoLabel, or Energy Star",
            "Choose brands that publish lifecycle assessment reports"
        ],
        "waste": [
            "Check packaging symbols before disposal",
            "Segregate dry and wet waste as per local guidelines"
        ]
    }

# -------------------- DOWNLOAD PDF REPORT --------------------
@app.post("/download-report")
def download_report(payload: dict):
    data = payload.get("result")

    if not data:
        return {"error": "No result data provided"}

    os.makedirs("reports", exist_ok=True)

    filename = f"EcoLens_Report_{uuid.uuid4().hex}.pdf"
    file_path = os.path.abspath(f"reports/{filename}")

    c = canvas.Canvas(file_path, pagesize=A4)
    width, height = A4

    # ---------------- HEADER ----------------
    c.setFont("Helvetica-Bold", 20)
    c.drawString(40, height - 50, "EcoLens â€“ Sustainability Analysis Report")

    c.setFont("Helvetica", 10)
    c.drawString(
        40,
        height - 70,
        "AI-based Greenwashing Detection & Credibility Assessment"
    )

    y = height - 110
    c.setFont("Helvetica", 11)

    # ---------------- HELPER FUNCTIONS ----------------
    def section(title):
        nonlocal y
        c.setFont("Helvetica-Bold", 13)
        c.drawString(40, y, title)
        y -= 18
        c.setFont("Helvetica", 11)

    def line(label, value):
        nonlocal y
        c.drawString(40, y, f"{label}: {value}")
        y -= 16

    def draw_bar(label, value):
        nonlocal y
        value = min(max(int(value), 0), 100)
        c.drawString(40, y, f"{label} ({value}%)")
        y -= 10
        c.rect(40, y, 300, 10, stroke=1, fill=0)
        c.setFillColorRGB(0.1, 0.6, 0.3)
        c.rect(40, y, 3 * value, 10, stroke=0, fill=1)
        c.setFillColorRGB(0, 0, 0)
        y -= 24

    def draw_circle_score(score):
        score = min(max(int(score), 0), 100)
        c.setFont("Helvetica-Bold", 12)
        c.drawString(380, y + 40, "Credibility Score")
        c.circle(420, y, 32, stroke=1, fill=0)
        c.setFont("Helvetica-Bold", 16)
        c.drawCentredString(420, y - 6, f"{score}%")

    # ---------------- CLAIM SUMMARY ----------------
    section("Claim Summary")
    line("Greenwashing Risk", data["risk"])
    line("Greenwashing Type", data["type"])

    draw_circle_score(data["credibility"])
    y -= 60

    # ---------------- SCORE ANALYSIS ----------------
    section("Score Analysis")
    draw_bar("Consumer Risk Level", data["consumerRisk"])
    draw_bar("Environmental Impact Confidence", data["envConfidence"])

    # ---------------- WHY FLAGGED ----------------
    section("Why This Claim Was Flagged")
    for r in data["reasons"]:
        c.drawString(60, y, f"- {r}")
        y -= 14

    y -= 12

    # ---------------- ALTERNATIVES ----------------
    section("Recommended Eco-Friendly Alternatives")
    for a in data["alternatives"]:
        c.drawString(60, y, f"- {a}")
        y -= 14

    y -= 12

    # ---------------- WASTE ----------------
    section("Waste Awareness & Disposal Guidance")
    for w in data["waste"]:
        c.drawString(60, y, f"- {w}")
        y -= 14

    # ---------------- FOOTER ----------------
    c.setFont("Helvetica-Oblique", 9)
    c.drawString(
        40,
        40,
        "Generated by EcoLens | Explainable AI for Sustainability Transparency"
    )

    c.save()
    return FileResponse(file_path, filename=filename)


# -------------------- SHARE RESULT --------------------
shared_results = {}

@app.post("/share-result")
def share_result(payload: dict):
    result = payload.get("result")

    if not result:
        return {"error": "No result provided"}

    share_id = uuid.uuid4().hex[:8]
    shared_results[share_id] = result

    return {
        "share_url": f"http://127.0.0.1:8000/shared/{share_id}"
    }

@app.get("/shared/{share_id}")
def view_shared_result(share_id: str):
    result = shared_results.get(share_id)
    if not result:
        return {"error": "Shared result not found"}
    return result

# -------------------- QR ANALYSIS --------------------
@app.get("/analyze-qr/{qr_id}")
def analyze_qr(qr_id: str):
    row = df[df["qr_code_id"] == qr_id]

    if row.empty:
        return {"error": "QR code not found"}

    record = row.iloc[0]

    return {
        "risk": f"{record.greenwashing_risk} Greenwashing Risk",
        "reasons": [record.why_flagged],
        "type": record.greenwashing_type,
        "consumerRisk": int(record.consumer_risk_score),
        "envConfidence": int(record.environmental_confidence),
        "credibility": int(record.credibility_score),
        "alternatives": [record.eco_alternatives],
        "waste": [
            f"Waste Type: {record.waste_type}",
            record.waste_guidance
        ]
    }

# -------------------- COMPARE CLAIMS --------------------
@app.post("/compare-claims")
def compare_claims(payload: dict):
    claim_a = payload.get("claimA", "")
    claim_b = payload.get("claimB", "")

    result_a = analyze_claim_text(claim_a)
    result_b = analyze_claim_text(claim_b)

    if result_a["credibility_score"] > result_b["credibility_score"]:
        verdict = "Claim A is more credible than Claim B"
    elif result_a["credibility_score"] < result_b["credibility_score"]:
        verdict = "Claim B is more credible than Claim A"
    else:
        verdict = "Both claims have similar credibility"

    return {
        "claimA": {
            "risk": result_a["risk"],
            "type": result_a["greenwashing_type"],
            "credibility": int(result_a["credibility_score"]),
            "consumerRisk": int(result_a["consumer_risk_score"]),
            "envConfidence": int(result_a["environmental_confidence"])
        },
        "claimB": {
            "risk": result_b["risk"],
            "type": result_b["greenwashing_type"],
            "credibility": int(result_b["credibility_score"]),
            "consumerRisk": int(result_b["consumer_risk_score"]),
            "envConfidence": int(result_b["environmental_confidence"])
        },
        "verdict": verdict
    }
@app.post("/analyze-url")
def analyze_product_url(payload: dict):
    url = payload.get("url", "")

    if not url:
        return {"error": "URL not provided"}

    try:
        headers = {
            "User-Agent": "Mozilla/5.0"
        }

        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")

        # Remove scripts & styles
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()

        # Try extracting product-related text
        texts = []

        for tag in soup.find_all(["p", "span", "li", "h1", "h2", "h3"]):
            content = tag.get_text(" ", strip=True)
            if len(content) > 40:
                texts.append(content)

        text = " ".join(texts)[:4000]

        if not text:
            return {"error": "Could not extract product info"}

        ai_result = analyze_claim_text(text)

        return {
            "risk": ai_result["risk"],
            "reasons": [ai_result["why_flagged"]],
            "type": ai_result["greenwashing_type"],
            "consumerRisk": ai_result["consumer_risk_score"],
            "envConfidence": ai_result["environmental_confidence"],
            "credibility": ai_result["credibility_score"],
            "alternatives": [
                "Prefer certified eco products",
                "Check brand sustainability reports"
            ],
            "waste": [
                "Check packaging disposal symbols",
                "Follow recycling instructions"
            ]
        }

    except Exception:
        return {"error": "Failed to fetch product page"}
